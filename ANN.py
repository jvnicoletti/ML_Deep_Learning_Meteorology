# -*- coding: utf-8 -*-
"""1.rede_neural.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qejiQeP8NUVbiI0NP4_DN4qMf9kYQ_ns
"""

# Importação das bibliotecas
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix,accuracy_score
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.utils import np_utils
from pandas import DataFrame
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sn

# Carregamento da base de dados e criação dos previsores (variáveis independentes - X) e da classe (variável dependente - y)
df = pd.read_csv('G:/My Drive/Estatística/Dados/Cluster_Final/K&G/Serie/Dados/ETP_ANUAL_SERIE.csv',encoding = 'utf_8')
#df = df.loc[df['Estacao'] == "Inverno"]
#df = df.loc[df['NOAA'] == "PADRAO"]

names = df.columns
df.head()

previsores1 = pd.DataFrame(df.iloc[:,1:29].values)
previsores2 = pd.DataFrame(df.iloc[:,31].values)
cidades = pd.DataFrame(df.iloc[:,32].values)
anos = pd.DataFrame(df.iloc[:,0].values)
previsores = pd.concat([previsores1, previsores2,cidades,anos], axis=1)

names = df.columns[1:29].tolist() + [df.columns[31],df.columns[32],df.columns[0]]

previsores

previsores = previsores.values

# Transformação dos atributos categóricos em atributos numéricos, passando o índice de cada coluna categórica

labelencoder1 = LabelEncoder()
labelencoder1 = LabelEncoder()

previsores[:,28] = labelencoder1.fit_transform(previsores[:,28])

previsores

classe = df.iloc[:,30].values
classe
labelencoder2 = LabelEncoder()
classe = labelencoder2.fit_transform(classe)
classe
quantidade2 = np.unique(classe, return_counts = True)
quantidade2

# Transformação da classe para o formato "dummy", pois temos uma rede neural com 3 neurônios na camada de saída
classe_dummy = np_utils.to_categorical(classe)
classe_dummy

# Divisão da base de dados entre treinamento e teste (30% para testar e 70% para treinar)
X_treinamento, X_teste, y_treinamento, y_teste = train_test_split(previsores,
                                                                  classe_dummy,
                                                                  test_size = 0.3,
                                                                  random_state = 0)

cidades_nomes =  X_teste[:, [29]]
anos_nomes =  X_teste[:, [30]]
anos = DataFrame(anos_nomes,columns = ["Anos"])
cidades = DataFrame(cidades_nomes,columns = ["Cidade"])
X_teste = np.delete(X_teste, np.s_[29], axis=1)
X_treinamento = np.delete(X_treinamento, np.s_[29], axis=1)
X_teste = np.delete(X_teste, np.s_[29], axis=1)
X_treinamento = np.delete(X_treinamento, np.s_[29], axis=1)

anos

X_treinamento = X_treinamento.astype('float32')
X_teste = X_teste.astype('float32')
y_treinamento = y_treinamento.astype('float32')
y_teste = y_teste.astype('float32')

# Criação da estrutura da rede neural com a classe Sequential (sequência de camadas)
modelo = Sequential()

#primeira camada oculta, 5 neuronios, 4 neuronios de entrada
modelo.add(Dense(units = 29, input_dim = 29,activation = 'relu'))
#segunda camada oculta
modelo.add(Dense(units = 19,activation = 'relu'))
#terceira camada oculta
modelo.add(Dense(units = 14,activation = 'relu'))
#quarta camada oculta
modelo.add(Dense(units = 11))

# Função softmax porque temos um problema de classificação com mais de duas classes 
#(é gerada uma probabilidade em cada neurônio)
modelo.add(Dense(units = 6, activation = 'softmax'))

# Visualização da estrutura da rede neural
modelo.summary()
modelo

# Configuração dos parâmetros da rede neural (adam = algoritmo para atualizar os pesos e loss = cálculo do erro)
modelo.compile( loss = "categorical_crossentropy", 
               optimizer = "adam", 
               metrics=['accuracy']
             )
# Treinamento, dividindo a base de treinamento em uma porção para validação (validation_data)
historico = modelo.fit(X_treinamento, y_treinamento, epochs = 1000,
           validation_data = (X_teste, y_teste))

# Previsões e mudar a variável para True ou False de acordo com o threshold 0.5
previsoes = modelo.predict(X_teste)
previsoes = (previsoes > 0.5)
previsoes

# Como é um problema com três saídas, precisamos buscar a posição que possui o maior valor (são retornados 3 valores)
y_teste_matrix = [np.argmax(t) for t in y_teste]
y_previsao_matrix = [np.argmax(t) for t in previsoes]

Previsao = DataFrame(y_previsao_matrix,columns=['Previsao'])
Testemunha = DataFrame(y_teste_matrix,columns=['Testemunha'])
Previsao = DataFrame(labelencoder2.inverse_transform(y_previsao_matrix),columns=['Previsao'])
Testemunha = DataFrame(labelencoder2.inverse_transform(y_teste_matrix),columns=['Testemunha'])
df_classificacao = pd.concat([Previsao, Testemunha,cidades,anos], axis=1)
df_classificacao
df_classificacao.to_csv(r'G:/My Drive/Estatística/Dados/Cluster_Final/K&G/Serie/Dados/Tabela_Comparacao.csv')

# Geração da matriz de confusão
confusao = confusion_matrix(y_teste_matrix, y_previsao_matrix)
confusao

df_cm = pd.DataFrame(confusao,index = sorted(pd.unique(Testemunha["Testemunha"])),columns = sorted(pd.unique(Testemunha["Testemunha"])))
df_cm
plt.figure(figsize = (10,7))
sn.heatmap(df_cm, annot=True,fmt='g')

# Gráfico para visualizar os erros e accuracy
#The training loss is that the average of the losses over every batch of training knowledge. 
#Because your model is changing over time, the loss over the first batches of an epoch is generally higher than over the last batches. 
#On the other hand, the testing loss for an epoch is computed using the model because it is at the tip of the epoch, leading to a lower loss.
historico.history.keys()
#evolução do erro, azul
plt.plot(historico.history['val_loss'])
#performance da rede
plt.plot(historico.history['val_accuracy'])

